Caught DGFException on creation of SGrid, trying default DGF method!

Created parallel ALUGrid<3,3,cube,nonconforming> from macro grid file ''. 

globalRefine: 0
elements = 7314938 (228123,228988,1.00379)   maxLevel = 3   step = 12   time = 0.108   dt = 0.009
elements = 7203722 (224702,225523,1.00365)   maxLevel = 3   step = 23   time = 0.207   dt = 0.009
elements = 7158376 (223229,224057,1.00371)   maxLevel = 3   step = 34   time = 0.306   dt = 0.009
elements = 7317696 (228239,229112,1.00382)   maxLevel = 3   step = 45   time = 0.405   dt = 0.009
elements = 7051325 (219854,220699,1.00384)   maxLevel = 3   step = 56   time = 0.504   dt = 0.009
elements = 7309597 (228009,228789,1.00342)   maxLevel = 3   step = 67   time = 0.603   dt = 0.009
elements = 7206172 (224869,225619,1.00334)   maxLevel = 3   step = 78   time = 0.702   dt = 0.009
elements = 7122977 (222107,223094,1.00444)   maxLevel = 3   step = 89   time = 0.801   dt = 0.009
elements = 7305565 (227846,228715,1.00381)   maxLevel = 3   step = 100   time = 0.9   dt = 0.009
elements = 7044052 (219667,220558,1.00406)   maxLevel = 3   step = 112   time = 1.008   dt = 0.009
U2 max: 115.85  min: 85.29
U3 max: 0.01  min: 0
U4 max: 16.56  min: 15.52
U5 max: 24.73  min: 11.74
U6 max: 0  min: 0
U7 max: 116.6  min: 85.94
U8 max: 135.16  min: 104.04
Program finished: CPU time = 454.66 sec.
Job  /ncar/opt/lsf/8.3/linux2.6-glibc2.3-x86_64/bin/poejob ./main_ball 4 0 3 none

TID   HOST_NAME   COMMAND_LINE            STATUS            TERMINATION_TIME
===== ========== ================  =======================  ===================
00016 ys2855-ib  ./main_ball 4 0   Done                     05/15/2013 16:12:36
00017 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00018 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00020 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00019 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00021 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00022 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00023 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00024 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00025 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00026 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00027 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00028 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00030 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00029 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00031 ys2855-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00000 ys2854-ib  ./main_ball 4 0   Done                     05/15/2013 16:12:36
00014 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00003 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00015 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00004 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00005 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00006 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00007 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00008 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00009 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00010 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00011 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00012 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00001 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00013 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36
00002 ys2854-ib   ./main_ball 4 0  Done                     05/15/2013 16:12:36

------------------------------------------------------------
Sender: LSF System <lsfadmin@ys2854-ib>
Subject: Job 662502: <mb.32> Done

Job <mb.32> was submitted from host <yslogin3-ib> by user <robertk> in cluster <yellowstone>.
Job was executed on host(s) <16*ys2854-ib>, in queue <small>, as user <robertk> in cluster <yellowstone>.
                            <16*ys2855-ib>
</glade/u/home/robertk> was used as the home directory.
</glade/u/home/robertk/work/Dune/trunk/alugrid/examples> was used as the working directory.
Started at Wed May 15 16:04:53 2013
Results reported at Wed May 15 16:12:39 2013

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# LSF batch script to run an MPI application
#
#BSUB -P P86850055           # project code
#BSUB -W 00:30               # wall-clock time (hrs:mins)
#BSUB -n 32              # number of tasks in job
#BSUB -R span[ptile=16]    # run 16 MPI tasks per node
#BSUB -J mb.32            # job name
#BSUB -o ./out/mb.32.%J.out  # output file name 
#BSUB -e ./err/mb.32.%J.err  # error file name
#BSUB -q small             # queue
#run the executable
mpirun.lsf ./main_ball 4 0 3 none

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :  14585.66 sec.
    Max Memory :      8360 MB
    Max Swap   :     19168 MB

    Max Processes  :        23
    Max Threads    :       209

The output (if any) is above this job summary.



PS:

Read file <./err/mb.32.662502.err> for stderr output of this job.

