Execute poe command line: poe  ./main_ball 4 0 3 none
Caught DGFException on creation of SGrid, trying default DGF method!

Created parallel ALUGrid<3,3,cube,nonconforming> from macro grid file ''. 

globalRefine: 0
elements = 7303962 (29256,59915,2.04796)   maxLevel = 3   step = 12   time = 0.108   dt = 0.009
elements = 7201552 (30014,59075,1.96825)   maxLevel = 3   step = 23   time = 0.207   dt = 0.009
elements = 7149920 (14117,58651,4.15464)   maxLevel = 3   step = 34   time = 0.306   dt = 0.009
elements = 7316310 (36478,60016,1.64527)   maxLevel = 3   step = 45   time = 0.405   dt = 0.009
elements = 7044199 (35179,57784,1.64257)   maxLevel = 3   step = 56   time = 0.504   dt = 0.009
elements = 7302800 (24935,59905,2.40245)   maxLevel = 3   step = 67   time = 0.603   dt = 0.009
elements = 7199011 (22733,59054,2.59772)   maxLevel = 3   step = 78   time = 0.702   dt = 0.009

------------------------------------------------------------
Sender: LSF System <lsfadmin@ys6314-ib>
Subject: Job 932228: <mb.128> in cluster <yellowstone> Exited

Job <mb.128> was submitted from host <yslogin3-ib> by user <robertk> in cluster <yellowstone>.
Job was executed on host(s) <16*ys6314-ib>, in queue <small>, as user <robertk> in cluster <yellowstone>.
                            <16*ys6315-ib>
                            <16*ys6316-ib>
                            <16*ys6320-ib>
                            <16*ys6331-ib>
                            <16*ys6332-ib>
                            <16*ys6335-ib>
                            <16*ys6338-ib>
</glade/u/home/robertk> was used as the home directory.
</glade/u/home/robertk/work/Dune/trunk/alugrid/examples/run/mb_zgraph_403> was used as the working directory.
Started at Fri Feb 21 09:57:36 2014
Results reported at Fri Feb 21 10:28:38 2014

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# LSF batch script to run an MPI application
#
#BSUB -P P86850055           # project code
#BSUB -W 00:20               # wall-clock time (hrs:mins)
#BSUB -n 128              # number of tasks in job
#BSUB -R span[ptile=16]    # run 16 MPI tasks per node
#BSUB -J mb.128            # job name
#BSUB -o ./mb.128.out  # output file name 
#BSUB -e ./mb.128.err  # error file name
#BSUB -q small             # queue
#run the executable
export MP_CSS_INTERRUPT=yes
export MP_EAGER_LIMIT=4194305
export MP_EAGER_LIMIT_LOCAL=4194305
mpirun.lsf ./main_ball 4 0 3 none

------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited
The output (if any) is above this job summary.



PS:

Read file <./mb.128.err> for stderr output of this job.

